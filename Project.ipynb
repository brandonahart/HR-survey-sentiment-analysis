{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CNF5N1nI2QqN"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load your dataset\n",
        "df = pd.read_csv('employee_reviews.csv', encoding='latin-1')\n",
        "\n",
        "# Columns to drop\n",
        "columns_to_drop = [\n",
        "    'company', 'location', 'dates', 'job-title', 'overall-ratings',\n",
        "    'work-balance-stars', 'culture-values-stars', 'carrer-opportunities-stars',\n",
        "    'comp-benefit-stars', 'senior-mangemnet-stars', 'helpful-count', 'link'\n",
        "]\n",
        "\n",
        "# Drop the unwanted columns\n",
        "df.drop(columns=columns_to_drop, inplace=True)\n",
        "\n",
        "df['advice-to-mgmt'] = df['advice-to-mgmt'].apply(lambda x: '' if str(x).strip().lower() == 'none' else str(x).strip())\n",
        "\n",
        "# Combine text fields into one column\n",
        "df['full_text'] = df[['summary', 'pros', 'cons', 'advice-to-mgmt']].fillna('').agg('. '.join, axis=1)\n",
        "\n",
        "# Drop the original text columns\n",
        "df.drop(columns=['summary', 'pros', 'cons', 'advice-to-mgmt'], inplace=True)\n",
        "\n",
        "# Save cleaned dataset\n",
        "df.to_csv('cleaned_dataset.csv', index=False)\n",
        "\n",
        "print(\"Dataset cleaned and saved as 'cleaned_dataset.csv'\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets nltk scikit-learn -q\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import nltk\n",
        "import re\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from datasets import Dataset\n",
        "from nltk import pos_tag, word_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "metadata": {
        "id": "1QWLqDwL8o9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load and Clean Dataset\n",
        "df = pd.read_csv(\"cleaned_dataset_with_sentiment.csv\")\n",
        "df = df.dropna(subset=[\"full_text\"])\n",
        "df = df.reset_index(drop=True)\n",
        "\n",
        "def pos_chunk_text(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    tagged = pos_tag(tokens)\n",
        "    return \" \".join([f\"{word}/{tag}\" for word, tag in tagged])\n",
        "\n",
        "df['pos_tagged'] = df['full_text'].apply(pos_chunk_text)"
      ],
      "metadata": {
        "id": "Zc-qpOy-DZYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenization and Dataset Split\n",
        "def tokenize_dataset(df, tokenizer):\n",
        "    dataset = Dataset.from_pandas(df[['full_text', 'sentiment_score','label']])\n",
        "    def tokenize_fn(example):\n",
        "        return tokenizer(example['full_text'], padding=\"max_length\", truncation=True, max_length=512)\n",
        "    return dataset.map(tokenize_fn, batched=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "KU8xlwIiDfFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load Models and Tokenizers\n",
        "model_names = {\n",
        "    \"roberta-base\": \"roberta-base\",\n",
        "    \"twitter-roberta\" : \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
        "}\n",
        "\n",
        "models = {}\n",
        "tokenizers = {}\n",
        "\n",
        "for name, path in model_names.items():\n",
        "    tokenizers[name] = AutoTokenizer.from_pretrained(path)\n",
        "    models[name] = AutoModelForSequenceClassification.from_pretrained(path, num_labels=3).to(device)\n",
        "\n",
        "def convert_score_to_class(score):\n",
        "    if score <= -0.33:\n",
        "        return 0  # Negative\n",
        "    elif score <= 0.33:\n",
        "        return 1  # Neutral\n",
        "    else:\n",
        "        return 2  # Positive\n",
        "\n",
        "df['label'] = df['sentiment_score'].apply(convert_score_to_class)\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "QOrbav7PDhzG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_df)"
      ],
      "metadata": {
        "id": "TlGlaplvg6CU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train Function\n",
        "def train_model(model_name, fine_tune=False):\n",
        "    tokenizer = tokenizers[model_name]\n",
        "    model = models[model_name]\n",
        "\n",
        "    train_dataset = tokenize_dataset(train_df, tokenizer)\n",
        "    test_dataset = tokenize_dataset(test_df, tokenizer)\n",
        "\n",
        "    train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "    test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "\n",
        "    if fine_tune:\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir=f'./results_{model_name}',\n",
        "            eval_strategy=\"epoch\",\n",
        "            save_strategy=\"epoch\",\n",
        "            learning_rate=2e-5,\n",
        "            per_device_train_batch_size=8,\n",
        "            per_device_eval_batch_size=8,\n",
        "            num_train_epochs=3,\n",
        "            weight_decay=0.01,\n",
        "            logging_dir=f'./logs_{model_name}',\n",
        "            logging_steps=10,\n",
        "            load_best_model_at_end=True,\n",
        "            report_to=\"none\",\n",
        "            fp16=True\n",
        "        )\n",
        "\n",
        "        trainer = Trainer(\n",
        "            model=model,\n",
        "            args=training_args,\n",
        "            train_dataset=train_dataset,\n",
        "            eval_dataset=test_dataset,\n",
        "            tokenizer=tokenizer,\n",
        "        )\n",
        "\n",
        "        trainer.train()\n",
        "\n",
        "    return model, tokenizer, test_dataset\n"
      ],
      "metadata": {
        "id": "pr4OrWWcDnHT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluation Function\n",
        "def evaluate(model, tokenizer, dataset):\n",
        "    model.eval()\n",
        "    preds, labels = [], []\n",
        "    for item in dataset:\n",
        "        input_ids = item[\"input_ids\"].unsqueeze(0).to(device)\n",
        "        attention_mask = item[\"attention_mask\"].unsqueeze(0).to(device)\n",
        "        with torch.no_grad():\n",
        "            output = model(input_ids, attention_mask=attention_mask)\n",
        "        preds.append(torch.argmax(output.logits, dim=1).cpu().item())\n",
        "        labels.append(item[\"label\"])\n",
        "\n",
        "    print(classification_report(labels, preds, target_names=[\"negative\", \"neutral\", \"positive\"]))\n",
        "    print(\"Accuracy:\", accuracy_score(labels, preds))\n",
        "    print(\"F1 Score:\", f1_score(labels, preds, average='weighted'))\n",
        "    return labels, preds\n"
      ],
      "metadata": {
        "id": "yTplWiWTDwoU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "DZo8Eot8fa2C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Error Analysis\n",
        "def error_analysis(df, labels, preds):\n",
        "    errors = []\n",
        "    for i, (l, p) in enumerate(zip(labels, preds)):\n",
        "\n",
        "        errors.append({\n",
        "            'Text': test_df.iloc[i]['full_text'],\n",
        "            'Predicted': p,\n",
        "            'True Label': l,\n",
        "            'Sentiment Score': test_df.iloc[i]['sentiment_score'],\n",
        "\n",
        "        })\n",
        "    return pd.DataFrame(errors)"
      ],
      "metadata": {
        "id": "chAeIc2DD4Hl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train & Compare Models\n",
        "all_results = {}\n",
        "for model_name in model_names:\n",
        "    print(f\"\\nðŸš€ Evaluating: {model_name} (pretrained only)\")\n",
        "    model, tokenizer, test_data = train_model(model_name, fine_tune=False)\n",
        "    labels, preds = evaluate(model, tokenizer, test_data)\n",
        "    all_results[f'{model_name}_pretrained'] = (labels, preds)\n",
        "\n",
        "    error_df = error_analysis(test_df.reset_index(), labels, preds)\n",
        "    error_df['True Label'] = error_df['True Label'].astype(int)\n",
        "    error_df.to_csv(f'error_analysis_{model_name}_pretrained.csv', index=False)\n",
        "\n",
        "\n",
        "    print(f\"\\nðŸŽ¯ Fine-tuning: {model_name}\")\n",
        "    model, tokenizer, test_data = train_model(model_name, fine_tune=True)\n",
        "    labels, preds = evaluate(model, tokenizer, test_data)\n",
        "    all_results[f'{model_name}_finetuned'] = (labels, preds)\n",
        "\n",
        "    error_df = error_analysis(test_df.reset_index(), labels, preds)\n",
        "    error_df['True Label'] = error_df['True Label'].astype(int)\n",
        "    error_df.to_csv(f'error_analysis_{model_name}_finetuned.csv', index=False)\n"
      ],
      "metadata": {
        "id": "d6e7EFOBDz9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# manual testing\n",
        "#checkpoint_path = \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
        "checkpoint_path = \"./drive/MyDrive/results_twitter-roberta/checkpoint-20259\"\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint_path).to(device)\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)\n",
        "\n",
        "def predict_sentiment(text, model, tokenizer):\n",
        "    inputs = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\", max_length=512).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    logits = outputs.logits\n",
        "    predicted_class = torch.argmax(logits, dim=1).item()\n",
        "\n",
        "    sentiment_labels = [\"negative\", \"neutral\", \"positive\"]\n",
        "    sentiment = sentiment_labels[predicted_class]\n",
        "\n",
        "    sentiment_score = torch.softmax(logits, dim=1).squeeze().cpu().numpy()\n",
        "\n",
        "    ranking = np.argsort(sentiment_score)\n",
        "    ranking = ranking[::-1]\n",
        "    for i in range(sentiment_score.shape[0]):\n",
        "        l = sentiment_labels[ranking[i]]\n",
        "        s = sentiment_score[ranking[i]]\n",
        "        print(f\"{i+1}) {l} {np.round(float(s), 4)}\")\n",
        "\n",
        "    return sentiment, sentiment_score\n",
        "\n",
        "#Example of how to use the function\n",
        "custom_text = \"Leaving whilst its dark is fun. #not #sucks\"\n",
        "sentiment, sentiment_score = predict_sentiment(custom_text, model, tokenizer)\n",
        "\n",
        "print(f\"Sentiment: {sentiment}\")\n",
        "print(f\"Sentiment Scores: {sentiment_score}\")"
      ],
      "metadata": {
        "id": "B6XebYR_lscE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}